<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.5/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.5/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.5/dist/index.js"></script><script>(()=>{setTimeout(()=>{const{markmap:x,mm:K}=window,P=new x.Toolbar;P.attach(K);const F=P.render();F.setAttribute("style","position:absolute;bottom:20px;right:20px"),document.body.append(F)})})()</script><script>((b,L,T,D)=>{const H=b();window.mm=H.Markmap.create("svg#mindmap",(L||H.deriveOptions)(D),T)})(()=>window.markmap,null,{"content":"Large Language Models (LLMs) - Revisão Detalhada","children":[{"content":"1. Conceitos Básicos","children":[{"content":"<pre data-lines=\"7,29\"><code>### 1.1. Definição de LLMs\n    * Modelos de IA avançados para linguagem natural.\n    * Treinados em vastos conjuntos de dados textuais.\n    * Baseados em Deep Learning e Redes Neurais Profundas.\n### 1.2. Aplicações Práticas\n    #### 1.2.1. Geração de Texto\n        * Criação de textos originais (artigos, e-mails, etc).\n    #### 1.2.2. Tradução de Idiomas\n        * Tradução automática entre várias línguas.\n    #### 1.2.3. Resumo de Textos\n        * Criação de resumos concisos de textos extensos.\n    #### 1.2.4. Respostas a Perguntas\n        * Fornecer respostas com base em informações textuais.\n    #### 1.2.5. Classificação de Texto\n        * Categorização de textos (sentimentos, tópicos, etc.).\n    #### 1.2.6. Conclusão de Textos\n        * Preencher lacunas ou finalizar trechos incompletos.\n    #### 1.2.7. Geração de Código\n        * Criação de código de programação em diferentes linguagens.\n### 1.3. Aprendizado de Máquina\n    * Utilizam aprendizado supervisionado, não supervisionado e auto-supervisionado.\n    * Aprendizado através da previsão da próxima palavra ou completar texto.\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"7,29"}}],"payload":{"tag":"h2","lines":"6,7"}},{"content":"2. Arquitetura dos LLMs","children":[{"content":"<pre data-lines=\"31,56\"><code>### 2.1. Arquitetura Transformer\n    * Base da maioria dos LLMs modernos.\n    * Solução para problemas de dependências de longo alcance em sequências.\n    * Utiliza mecanismos de atenção (attention mechanism).\n### 2.2. Componentes Chave\n    #### 2.2.1. Embedding Layer\n        * Converte palavras (tokens) em vetores numéricos densos.\n        * Representa o significado semântico das palavras.\n    #### 2.2.2. Encoder\n        * Processa a sequência de entrada para representação contextual.\n        * Usa Multi-Head Attention e Feed-Forward Networks.\n    #### 2.2.3. Decoder\n        * Gera a sequência de saída (texto) a partir das representações do Encoder.\n        * Opera de forma auto-regressiva (gera token a token).\n    #### 2.2.4. Normalization Layer\n        * Normaliza as saídas das camadas para estabilidade do treinamento.\n        * Exemplos: Layer Normalization, Batch Normalization.\n    #### 2.2.5. Output Layer\n        * Converte a saída em probabilidades para cada token.\n        * Escolhe o token mais provável como saída.\n### 2.3. Mecanismos de Atenção\n    * Permite que o modelo foque em diferentes partes da sequência de entrada.\n    * Multi-Head Attention: Atenção paralela em vários aspectos do texto.\n### 2.4. Feed-Forward Networks\n    * Aplicam transformações não-lineares para aumentar capacidade de representação.\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"31,56"}}],"payload":{"tag":"h2","lines":"30,31"}},{"content":"3. Processo de Treinamento","children":[{"content":"<pre data-lines=\"58,65\"><code>### 3.1. Pré-Treinamento (Pre-training)\n    * Treinamento em grandes volumes de dados textuais não rotulados.\n    * Objetivos: aprender a estrutura, gramática e conhecimento geral.\n    * Tarefas típicas: previsão de máscara (Masked Language Modeling - BERT) e previsão do próximo token (GPT).\n### 3.2. Ajuste Fino (Fine-tuning)\n    * Adaptação do modelo pré-treinado para tarefas específicas.\n    * Utilização de dados rotulados para otimizar o desempenho.\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"58,65"}}],"payload":{"tag":"h2","lines":"57,58"}},{"content":"4. Classificações dos LLMs","children":[{"content":"<pre data-lines=\"67,85\"><code>### 4.1. Baseados em Arquitetura\n    #### 4.1.1. Encoder-Only Models\n        * Utilizam apenas o encoder do Transformer.\n        * Especializados em classificação e extração de informações.\n        * Exemplos: BERT, RoBERTa, ALBERT.\n    #### 4.1.2. Decoder-Only Models\n        * Utilizam apenas o decoder do Transformer.\n        * Especializados em geração de texto e tradução.\n        * Exemplos: GPT (GPT-2, GPT-3, GPT-4).\n    #### 4.1.3. Encoder-Decoder Models\n        * Utilizam tanto o encoder quanto o decoder do Transformer.\n        * Especializados em tarefas de tradução, resumo e diálogos.\n        * Exemplos: T5, BART.\n### 4.2. Outras Classificações\n    * Modelos auto-regressivos vs. não auto-regressivos.\n    * Modelos baseados em atributos (conhecimento, código, etc.).\n    * Modelos multi-modais (texto, imagem, áudio, etc.).\n    * Modelos open-source vs. proprietary.\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"67,85"}}],"payload":{"tag":"h2","lines":"66,67"}},{"content":"5. Camadas dos LLMs (Abstração)","children":[{"content":"<pre data-lines=\"87,96\"><code>### 5.1. Camada de Entrada\n    * Recebe o texto, realiza tokenização e transforma em embeddings.\n### 5.2. Camada de Transformação\n    * Realiza o processamento com mecanismos de atenção e feed-forward networks.\n### 5.3. Camada de Saída\n    * Gera a probabilidade para cada token no vocabulário e escolhe o mais provável.\n### 5.4. Abstração do Funcionamento\n    * O funcionamento complexo é abstraído para facilitar a utilização.\n    * O foco principal é na entrada (input) e saída (output) de texto.\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"87,96"}}],"payload":{"tag":"h2","lines":"86,87"}},{"content":"6. Técnicas Chave","children":[{"content":"<pre data-lines=\"98,106\"><code>### 6.1. Attention Mechanisms\n    * Permite que o modelo relacione palavras dentro de um texto.\n### 6.2. Layer Normalization\n    * Acelera o treinamento e melhora a estabilidade.\n### 6.3. Positional Encoding\n    * Adiciona informação sobre a posição das palavras no texto.\n### 6.4. Dropout\n    * Técnica de regularização para evitar overfitting.\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"98,106"}}],"payload":{"tag":"h2","lines":"97,98"}}],"payload":{"tag":"h1","lines":"4,5"}},{"colorFreezeLevel":2})</script>
</body>
</html>
